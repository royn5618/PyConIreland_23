{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royn5618/PyConIreland_23/blob/main/colab_notebook_nlp_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E52eAfVEDc0"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvfIdtMEEDc3"
      },
      "source": [
        "**About the Dataset :** Natural Language Processing with Disaster Tweets is a Kaggle Challenge where tweets are collected with labels indicating whether the tweets are about a disaster that occurred or not. Since tweets are social media language, therefore, it is a challenge to automatically identify them. Besides, ambiguity in texts makes it more difficult to achieve automatic identification of tweets containing information on real disaster. The objective of this project is to predict using machine learning if a tweet contains information on occurrence of a real disaster or not.\n",
        "\n",
        "**Source :** https://www.kaggle.com/c/nlp-getting-started\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhBA8FW7EDc4"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "y5id_ihyEKl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:00:32.447439Z",
          "start_time": "2023-11-11T22:00:32.119998Z"
        },
        "id": "YcVWGS-qEDc5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.titlepad'] = 30\n",
        "\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize, TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from PIL import Image\n",
        "import PIL.ImageOps\n",
        "from wordcloud import ImageColorGenerator\n",
        "import contractions\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkSUslGEEDc9"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:56.391362Z",
          "start_time": "2023-11-11T21:58:56.105097Z"
        },
        "id": "1froqNBtEDc9"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('Data/train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-2-P6BGEDc-"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:56.523832Z",
          "start_time": "2023-11-11T21:58:56.406231Z"
        },
        "id": "ySnrAD0pEDc-"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:56.565925Z",
          "start_time": "2023-11-11T21:58:56.542904Z"
        },
        "id": "Ju3wAB8wEDc_"
      },
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNF9V6BAEDdA"
      },
      "source": [
        "**EDA Steps:**\n",
        "    \n",
        "1. Data Distribution\n",
        "2. Missing Values\n",
        "3. Cardinality for features and target\n",
        "4. Distribution of Target by keyword\n",
        "5. Distribution of Target by location\n",
        "6. Most frequently occurring words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2GdBOAzEDdA"
      },
      "source": [
        "## Cardinality Check\n",
        "\n",
        "How many unique values is present in each of the columns?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:56.759547Z",
          "start_time": "2023-11-11T21:58:56.715161Z"
        },
        "id": "eSUmb1U8EDdB"
      },
      "outputs": [],
      "source": [
        "for col in df_train.columns:\n",
        "    print(\"{} has {} unique instances\".format(col, len(df_train[col].unique())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tofn8gj2EDdB"
      },
      "source": [
        "There are 7613 unique ids which is in accordance with the dataframe shape.\n",
        "\n",
        "This is a binar classification task and 'Target' has two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:56.978492Z",
          "start_time": "2023-11-11T21:58:56.883971Z"
        },
        "id": "SwVKQXTEEDdC"
      },
      "outputs": [],
      "source": [
        "df_train.info(verbose=True, null_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WqKNAmCEDdC"
      },
      "source": [
        "## Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:58.913728Z",
          "start_time": "2023-11-11T21:58:57.107227Z"
        },
        "id": "d-7oQ7_4EDdC"
      },
      "outputs": [],
      "source": [
        "# Get the percentage of missing values\n",
        "output = df_train.isnull().sum() * 100  / len(df_train)\n",
        "# type(output) : Pandas Series\n",
        "\n",
        "# plot them\n",
        "plt.figure(figsize=[5,3])\n",
        "sns.barplot(y=list(output.index), x=list(output))\n",
        "plt.title('Missing Values by Columns')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYnBbWhcEDdD"
      },
      "source": [
        "Keyword and location has missing values. Location has 33% data missing. This missing data could be tackled by detecting location in tweets and filling them up. I will not be doing this since I will be using tf-idf on the tweet text joined with location. Therefore, this step is not required for modelling though could be useful for EDA.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:59.017023Z",
          "start_time": "2023-11-11T21:58:58.929454Z"
        },
        "id": "NPUfFnFNEDdD"
      },
      "outputs": [],
      "source": [
        "df_train[~df_train['location'].isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAM4a2hXEDdE"
      },
      "source": [
        "Evident that the location information is not consistant. It is also possible to extract location information from text as in id 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:59.076525Z",
          "start_time": "2023-11-11T21:58:59.029421Z"
        },
        "id": "eR0bCr4_EDdE"
      },
      "outputs": [],
      "source": [
        "df_train[df_train['id'] == 48]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "679Rv3jgEDdF"
      },
      "source": [
        "## Column Names - STATIC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6CE4ol1EDdF"
      },
      "source": [
        "I prefer standardizing column names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:58:59.136780Z",
          "start_time": "2023-11-11T21:58:59.106005Z"
        },
        "id": "C3Mao8GTEDdF"
      },
      "outputs": [],
      "source": [
        "KEYWORD = 'keyword'\n",
        "ID = 'id'\n",
        "LOCATION = 'location'\n",
        "TEXT = 'text'\n",
        "TARGET = 'target'\n",
        "TEXT_TOKENIZED = 'Text Tokenized'\n",
        "SENTIMENT = 'Sentiment Score'\n",
        "SENTIMENT_ROUND = 'Sentiment Score (rounded off)'\n",
        "WORDS_PER_TWEET = 'Words Per Tweet'\n",
        "CHAR_PER_TWEET = 'Characters Per Tweet'\n",
        "LOCATIONS = 'Locations'\n",
        "ALL_TEXT = 'all_text'\n",
        "ALL_TEXT_JOINED = 'all_text_joined'\n",
        "NUM_IN_TWEETS = 'Number in Tweet'\n",
        "PUNCTUATION_COUNT = 'Punctuation Count Per Tweet'\n",
        "IDENTIFIABLE_LOCATION = 'Identifiable Location'\n",
        "IN_BOW = 'Present In BOW'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cbSM5LyEDdF"
      },
      "source": [
        "## Keyword Analysis\n",
        "\n",
        "1. Which keywords have occurred the most?\n",
        "2. Which keywords have a higher percentage of tweets about real disasters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:13.727780Z",
          "start_time": "2023-11-11T21:58:59.150535Z"
        },
        "scrolled": false,
        "id": "olqDDJbBEDdG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[10, 60])\n",
        "sns.countplot(y=KEYWORD,\n",
        "              data=df_train,\n",
        "              palette=['grey'],\n",
        "              order=df_train[KEYWORD].value_counts().index)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Keyword Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:26.099953Z",
          "start_time": "2023-11-11T21:59:13.734722Z"
        },
        "scrolled": false,
        "id": "UWAIBYh_EDdG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[10, 60])\n",
        "sns.countplot(y=KEYWORD, hue=TARGET, data=df_train, palette=['grey', 'red'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(\"Distribution of Target per Keyword \")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Y35Ta2EDdH"
      },
      "source": [
        "Blank spaces indicated by %20. Will require to clean this.\n",
        "\n",
        "From this chart, it is seen that 'derailment', 'debris' and 'wreckage' are all about real disaster tweets.\n",
        "\n",
        "Body20%bags contains the higest difference between real and non-real disaster tweets where the number of non-real disaster tweets is high. It is actually the highest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:26.135250Z",
          "start_time": "2023-11-11T21:59:26.105973Z"
        },
        "id": "om-jXCjwEDdH"
      },
      "outputs": [],
      "source": [
        "df_train[(df_train[KEYWORD] == 'body%20bags') & (df_train[TARGET] == 1)][TEXT].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:26.192060Z",
          "start_time": "2023-11-11T21:59:26.143847Z"
        },
        "id": "6uufgR-KEDdI"
      },
      "outputs": [],
      "source": [
        "# Get keyword counts for tweets about real disasters\n",
        "real_disaster_keywords = df_train[df_train['target'] == 1].groupby(['keyword', 'target']).count()['id'].reset_index()\n",
        "real_disaster_keywords.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:26.293554Z",
          "start_time": "2023-11-11T21:59:26.200603Z"
        },
        "id": "HobJP7zmEDdI"
      },
      "outputs": [],
      "source": [
        "# Get keyword counts for tweets not about real disasters\n",
        "unreal_disaster_keywords = df_train[df_train['target'] == 0].groupby(['keyword', 'target']).count()['id'].reset_index()\n",
        "unreal_disaster_keywords.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:27.656690Z",
          "start_time": "2023-11-11T21:59:26.317704Z"
        },
        "id": "Noh2Yp-WEDdI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[6, 5])\n",
        "sns.barplot(x=KEYWORD, y=ID, data=real_disaster_keywords.sort_values('id', ascending=False)[:10], palette='gist_heat')\n",
        "plt.xticks(rotation=60)\n",
        "plt.ylabel('Count')\n",
        "plt.title('Top 10 Keywords for Tweets about Real Disasters')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:28.743270Z",
          "start_time": "2023-11-11T21:59:27.668094Z"
        },
        "id": "HsUEQrN8EDdI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[6, 5])\n",
        "sns.barplot(x=KEYWORD, y=ID, data=unreal_disaster_keywords.sort_values('id', ascending=False)[:10], palette='gist_heat')\n",
        "plt.xticks(rotation=60)\n",
        "plt.ylabel('Count')\n",
        "plt.title('Top 10 Keywords for Tweets NOT about Real Disasters')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:28.835545Z",
          "start_time": "2023-11-11T21:59:28.748824Z"
        },
        "id": "3izUHAHdEDdJ"
      },
      "outputs": [],
      "source": [
        "# Merge the counts and get calculate the probabilities that\n",
        "# if a particular keyword appears in a tweet, what is the probability that it is about a real disaster.\n",
        "\n",
        "merged_counts_keywords = pd.merge(\n",
        "    left=real_disaster_keywords,\n",
        "    right=unreal_disaster_keywords,\n",
        "    left_on=KEYWORD,\n",
        "    right_on=KEYWORD,\n",
        "    how='outer').drop(columns=['target_x', 'target_y']).fillna(0)\n",
        "\n",
        "merged_counts_keywords['prob_real_disasters'] = (\n",
        "    merged_counts_keywords['id_x'] -\n",
        "    merged_counts_keywords['id_y']) / merged_counts_keywords['id_x']\n",
        "\n",
        "top_prob_real_disaster_keywords = merged_counts_keywords.sort_values(\n",
        "    'prob_real_disasters', ascending=False)[:10]\n",
        "top_prob_real_disaster_keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGgpxtzSEDdJ"
      },
      "source": [
        "## Target Analysis\n",
        "\n",
        "**Imbalanced data** is a problem with classification tasks where the classes are not represented equally.Is the dataset balanced?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:28.890351Z",
          "start_time": "2023-11-11T21:59:28.846397Z"
        },
        "id": "jJfapjlmEDdK"
      },
      "outputs": [],
      "source": [
        "df_train.target.value_counts() / len(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:29.685282Z",
          "start_time": "2023-11-11T21:59:28.900338Z"
        },
        "id": "hLRm-jZ-EDdK"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=TARGET,\n",
        "              data=df_train[TARGET].replace({\n",
        "                  0: 'Not about Real Disaster',\n",
        "                  1: 'About Real Disaster'\n",
        "              }).reset_index(),\n",
        "              palette=['grey', 'red'])\n",
        "plt.title('Target Distribution')\n",
        "plt.ylabel(None)\n",
        "plt.xlabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_J1z9fEDdK"
      },
      "source": [
        "This dataset is slightly imbalanced which should not be a problem for us. The disprity is of ~1000 datapoints where the number of non-disastrous tweets are higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:30.348553Z",
          "start_time": "2023-11-11T21:59:29.694769Z"
        },
        "id": "MwZaIg_lEDdL"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=TARGET, data=df_train[~df_train[LOCATION].isna()], palette=['grey', 'red'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUFn5gmEDdL"
      },
      "source": [
        "For the tweets whose location is NOT missing, the data imabalance is true in this case as well. Therefore, dropping the location null data will not help balancing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:31.004777Z",
          "start_time": "2023-11-11T21:59:30.364718Z"
        },
        "id": "pM65w3m9EDda"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=TARGET, data=df_train[df_train[LOCATION].isna()], palette=['grey', 'red'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd9lShWcEDdb"
      },
      "source": [
        "For the tweets whose location is missing, the data imabalance is true in this case as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0KaTrPjEDdb"
      },
      "source": [
        "## Top 20 locations\n",
        "\n",
        "Some locations are more prone to real disasters and it is highly likely that tweets from those locations will be about real disasters.\n",
        "\n",
        "Objectives:\n",
        "\n",
        "1. Nature of location data\n",
        "2. Most frequently occurring locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:31.078268Z",
          "start_time": "2023-11-11T21:59:31.017715Z"
        },
        "id": "_ftOwI3UEDdb"
      },
      "outputs": [],
      "source": [
        "df_train[df_train[TARGET] == 1].groupby(LOCATION)[TARGET].count().reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb4znxGJEDdb"
      },
      "source": [
        "There are some\n",
        "* gibberish locations\n",
        "* latitudes and longitudes\n",
        "* english words in location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:31.148545Z",
          "start_time": "2023-11-11T21:59:31.087870Z"
        },
        "id": "kfn4J5l8EDdb"
      },
      "outputs": [],
      "source": [
        "df_train[df_train[TARGET] == 1].groupby(\n",
        "    LOCATION)[TARGET].count().reset_index().sort_values(by=TARGET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC9va5NFEDdc"
      },
      "source": [
        "There are -\n",
        "* city\n",
        "* city, state\n",
        "* city, country\n",
        "* country abbreviation\n",
        "* country name\n",
        "* city, country / worldwide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.095150Z",
          "start_time": "2023-11-11T21:59:31.164349Z"
        },
        "id": "hliG2XkMEDdc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[18, 10])\n",
        "sns.barplot(x=LOCATION,\n",
        "            y=TARGET,\n",
        "            data=df_train[df_train[TARGET] == 1].groupby(LOCATION)\n",
        "            [TARGET].count().reset_index().sort_values(by=TARGET,\n",
        "                                                       ascending=False)[:20],\n",
        "            palette='gist_heat'\n",
        "           )\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Top 20 Most Frequently Appearing Locations for Tweets about Real Disaster')\n",
        "plt.xlabel('Tweet Post Locations')\n",
        "plt.ylabel('Counts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUdBv_RnEDdc"
      },
      "source": [
        "There is an overlap of countries, cities and there are also co-ordinate information plus some gibberish data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlCGlJtHEDdc"
      },
      "source": [
        "## Text Data Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eDRqgzvEDdc"
      },
      "source": [
        "Here is have random tweet text checks  to see what is in there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.132974Z",
          "start_time": "2023-11-11T21:59:33.103728Z"
        },
        "id": "8WiPl876EDdd"
      },
      "outputs": [],
      "source": [
        "df_train[TEXT][8], df_train[TARGET][8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.161371Z",
          "start_time": "2023-11-11T21:59:33.142641Z"
        },
        "id": "aoQMf9cIEDdd"
      },
      "outputs": [],
      "source": [
        "df_train[TEXT][20], df_train[TARGET][20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.201092Z",
          "start_time": "2023-11-11T21:59:33.169951Z"
        },
        "id": "KdNlMktgEDdd"
      },
      "outputs": [],
      "source": [
        "df_train[TEXT][1000] , df_train[TARGET][1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.240453Z",
          "start_time": "2023-11-11T21:59:33.214570Z"
        },
        "id": "-r9URxxnEDde"
      },
      "outputs": [],
      "source": [
        "df_train[TEXT][2000] , df_train[TARGET][2000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxo8mcvFEDde"
      },
      "source": [
        "**Notes:**\n",
        "\n",
        "To remove:\n",
        "1. urls from the texts,\n",
        "2. html tags\n",
        "3. mentions using @.\n",
        "4. %20 from keywords\n",
        "\n",
        "Will retain hashtags since importance information lies in hasgtags but will remove the # in them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEmFwMfkEDde"
      },
      "source": [
        "# Basic Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OlbEAmuEDde"
      },
      "source": [
        "A regular expression or regex is a set of characters, or a pattern, which is used to find sub strings in a given string like getting urls, numbers, extracting all hashtags and mentions from a tweet from a large unstructured corpus.\n",
        "\n",
        "In python, re.sub does the job of substituting a detected pattern with a substitute in an input string. The syntax is as follows:\n",
        "\n",
        "```re.sub(pattern, replacement, input)```\n",
        "\n",
        "Other options are - findall, fullmatch, split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1c6xEeQEDdf"
      },
      "source": [
        "## Cleaning Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.274756Z",
          "start_time": "2023-11-11T21:59:33.249074Z"
        },
        "id": "qoT8fI8IEDdf"
      },
      "outputs": [],
      "source": [
        "test_string = 'I am at https://www.nabanita.org www.nabanita.org okay'\n",
        "url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "test_op = re.sub(url_pattern, '', test_string)\n",
        "test_op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.311836Z",
          "start_time": "2023-11-11T21:59:33.285008Z"
        },
        "id": "pIy-vPNIEDdf"
      },
      "outputs": [],
      "source": [
        "test_string = 'I am at <p>www.nabanita.org &nbsp;</p>'\n",
        "html_entities = r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n",
        "test_op = re.sub(html_entities, '', test_string)\n",
        "test_op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.351427Z",
          "start_time": "2023-11-11T21:59:33.327932Z"
        },
        "id": "GhqFItKMEDdf"
      },
      "outputs": [],
      "source": [
        "test_string = 'I am at @nabanita #python testing 123'\n",
        "html_entities = r'@([a-z0-9]+)|#'\n",
        "test_op = re.sub(html_entities, '', test_string)\n",
        "test_op"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thcnhOFAEDdf"
      },
      "source": [
        "A tweet might tag news channels as well which contains the word 'news'. If they are twitter handles, then the information will be lost. Hence adding this function to add the keyword news to the tweet if the word is present in the tweet text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9V1-wAcEDdg"
      },
      "source": [
        "## Text Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T21:59:33.405895Z",
          "start_time": "2023-11-11T21:59:33.363551Z"
        },
        "id": "wsYOu4jTEDdg"
      },
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    ''' This method takes in text to remove urls and website links, if any'''\n",
        "    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    text = re.sub(url_pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_html_entities(text):\n",
        "    ''' This method removes html tags'''\n",
        "    html_entities = r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n",
        "    text = re.sub(html_entities, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_lower_case(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def detect_news(text):\n",
        "    '''Appends news to the end of the tweet if news appears anywhere in the tweet.\n",
        "    This is to avoid missing out on the keyword 'news' if it occured in a mention, for ex: @SomeNewsChannel'''\n",
        "    if 'news' in text:\n",
        "        text = text + ' news'\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_social_media_tags(text):\n",
        "    ''' This method removes @ and # tags'''\n",
        "    tag_pattern = r'@([a-z0-9]+)|#'\n",
        "    text = re.sub(tag_pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Count it before I remove them altogether\n",
        "def count_punctuations(text):\n",
        "    getpunctuation = re.findall('[.?\"\\'`\\,\\-\\!:;\\(\\)\\[\\]\\\\/“”]+?', text)\n",
        "    return len(getpunctuation)\n",
        "\n",
        "\n",
        "def preprocess_text(x):\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\d\\s]+', '', x) ## Add nltk.word_tokenizer\n",
        "    word_list = []\n",
        "    for each_word in cleaned_text.split(' '):\n",
        "        word_list.append(contractions.fix(each_word).lower())\n",
        "    word_list = [\n",
        "        wnl.lemmatize(each_word.strip()) for each_word in word_list\n",
        "        if each_word not in STOPWORDS and each_word.strip() != ''\n",
        "    ]\n",
        "    return \" \".join(word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:06.517032Z",
          "start_time": "2023-11-11T22:00:42.582648Z"
        },
        "id": "1mRnzOM5EDdg"
      },
      "outputs": [],
      "source": [
        "df_train[TEXT] = df_train[TEXT].apply(remove_urls)\n",
        "df_train[TEXT] = df_train[TEXT].apply(remove_html_entities)\n",
        "df_train[TEXT] = df_train[TEXT].apply(convert_lower_case)\n",
        "df_train[TEXT] = df_train[TEXT].apply(detect_news)\n",
        "df_train[TEXT] = df_train[TEXT].apply(remove_social_media_tags)\n",
        "df_train[PUNCTUATION_COUNT] = df_train[TEXT].apply(count_punctuations)\n",
        "df_train[TEXT] = df_train[TEXT].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:06.916979Z",
          "start_time": "2023-11-11T22:01:06.868506Z"
        },
        "id": "KBzMWjfkEDdh"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:06.977759Z",
          "start_time": "2023-11-11T22:01:06.927724Z"
        },
        "id": "Y9y7jib9EDdh"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "# Expected to remove @FoxNews but have ' news' in the tweet text\n",
        "\n",
        "df_train[(df_train[KEYWORD] == 'body%20bags') & (df_train[TARGET] == 1)][TEXT].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2yHQgGVEDdh"
      },
      "source": [
        "## Punctuation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:07.971736Z",
          "start_time": "2023-11-11T22:01:06.993968Z"
        },
        "id": "MnIdTY0lEDdh"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=TARGET,\n",
        "            y=PUNCTUATION_COUNT,\n",
        "            data=df_train,\n",
        "            palette=['grey', 'red'])\n",
        "plt.title('Punctuation Analysis')\n",
        "plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Punctuation Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FZBnej6EDdi"
      },
      "source": [
        "## Keyword Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:08.874003Z",
          "start_time": "2023-11-11T22:01:08.845943Z"
        },
        "id": "H3rdgqnwEDdi"
      },
      "outputs": [],
      "source": [
        "def clean_keyword(text):\n",
        "    if text is not np.nan and text:\n",
        "        text = text.replace('%20', ' ')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:08.926026Z",
          "start_time": "2023-11-11T22:01:08.887736Z"
        },
        "id": "TCkX0nsPEDdi"
      },
      "outputs": [],
      "source": [
        "df_train[KEYWORD] = df_train[KEYWORD].apply(clean_keyword)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:08.977349Z",
          "start_time": "2023-11-11T22:01:08.939628Z"
        },
        "id": "vX21y_exEDdi"
      },
      "outputs": [],
      "source": [
        "df_train[KEYWORD].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYRsBBAcEDdi"
      },
      "source": [
        "# Mention of Numbers in Tweets\n",
        "\n",
        "Information on real disasters usually have a casualty count in numbers. Therefore, it is worth analyzing if numbers are present in the tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:09.010887Z",
          "start_time": "2023-11-11T22:01:08.993805Z"
        },
        "id": "a7NT4zyXEDdj"
      },
      "outputs": [],
      "source": [
        "def get_numbers_in_tweet(text):\n",
        "    list_numbers = re.findall(r'\\d+', text)\n",
        "    if list_numbers:\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:09.195678Z",
          "start_time": "2023-11-11T22:01:09.027096Z"
        },
        "id": "l-7LrJGjEDdj"
      },
      "outputs": [],
      "source": [
        "df_train[NUM_IN_TWEETS] = df_train[TEXT].apply(get_numbers_in_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:09.296133Z",
          "start_time": "2023-11-11T22:01:09.208436Z"
        },
        "id": "VxPEjzfYEDdj"
      },
      "outputs": [],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:01:10.484433Z",
          "start_time": "2023-11-11T22:01:09.307801Z"
        },
        "id": "b9kLGtezEDdj"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=TARGET, hue=NUM_IN_TWEETS, data=df_train)\n",
        "plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks = [0, 1])\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Appearance of Numbers in Tweets')\n",
        "plt.legend(labels=['Numbers Absent', 'Numbers Present'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzJ9QRIaEDdj"
      },
      "source": [
        "About 50% of the tweets about real disatsers have numbers present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB9-GR8MEDdk"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "The sentiment about real disasters would be generally negative. What is the assessed sentiment for these tweets?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacytextblob"
      ],
      "metadata": {
        "id": "vEtVBwLEE1CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:07:25.756663Z",
          "start_time": "2023-11-11T22:07:25.533971Z"
        },
        "id": "Cz54kDBfEDdk"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:11:21.166325Z",
          "start_time": "2023-11-11T22:11:16.289617Z"
        },
        "id": "K-knclTnEDdk"
      },
      "outputs": [],
      "source": [
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('spacytextblob')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:14:48.422706Z",
          "start_time": "2023-11-11T22:11:21.177822Z"
        },
        "id": "bjFdmwONEDdk"
      },
      "outputs": [],
      "source": [
        "df_train[SENTIMENT] = df_train[TEXT].apply(lambda x: nlp(x)._.polarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:14:48.494011Z",
          "start_time": "2023-11-11T22:14:48.431124Z"
        },
        "id": "GegXEGVTEDdk"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:14:55.024493Z",
          "start_time": "2023-11-11T22:14:48.891733Z"
        },
        "id": "PHjpY6AtEDdk"
      },
      "outputs": [],
      "source": [
        "sns.displot(x=SENTIMENT, hue=TARGET, data=df_train, kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikbpfHiuEDdl"
      },
      "source": [
        "The sentiment is a continuous number. How does it look like when we binarize it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:14:56.003861Z",
          "start_time": "2023-11-11T22:14:55.985144Z"
        },
        "id": "Rcdc3ALGEDdl"
      },
      "outputs": [],
      "source": [
        "def sentiment_to_binary(x):\n",
        "    if x > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:14:56.121317Z",
          "start_time": "2023-11-11T22:14:56.021600Z"
        },
        "id": "_7DTuYpjEDdl"
      },
      "outputs": [],
      "source": [
        "df_train[SENTIMENT_ROUND] = df_train[SENTIMENT].apply(sentiment_to_binary)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:14:56.174640Z",
          "start_time": "2023-11-11T22:14:56.131912Z"
        },
        "id": "cBI2nRBtEDdl"
      },
      "outputs": [],
      "source": [
        "df_train[SENTIMENT_ROUND].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:14:58.160444Z",
          "start_time": "2023-11-11T22:14:57.307191Z"
        },
        "scrolled": false,
        "id": "pss0aJSeEDdm"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=TARGET, hue=SENTIMENT_ROUND, data=df_train)\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xticks(labels=['Not Real Disasters', 'Real Disasters'], ticks=[0, 1])\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Counts')\n",
        "# plt.ylabel('Punctuation Count')\n",
        "plt.legend(labels=['Negative', 'Positive'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofp4AmliEDdm"
      },
      "source": [
        "The percentage of negative sentiment is more in tweets about real disasters than in tweets not about real disasters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG0IYSl6EDdm"
      },
      "source": [
        "# Tweet Length Analysis\n",
        "\n",
        "Are the tweets about real disasters shorter or longer in nature?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:15:09.557367Z",
          "start_time": "2023-11-11T22:15:05.312988Z"
        },
        "id": "Yu_-DCeWEDdm"
      },
      "outputs": [],
      "source": [
        "df_train[TEXT_TOKENIZED] = df_train[TEXT].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:15:09.656293Z",
          "start_time": "2023-11-11T22:15:09.568267Z"
        },
        "id": "ynpYb5POEDdn"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:15:09.723869Z",
          "start_time": "2023-11-11T22:15:09.674190Z"
        },
        "id": "u5h8z_nUEDdn"
      },
      "outputs": [],
      "source": [
        "df_train[WORDS_PER_TWEET] = df_train[TEXT_TOKENIZED].apply(len)\n",
        "df_train[CHAR_PER_TWEET] = df_train[TEXT].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:15:13.463069Z",
          "start_time": "2023-11-11T22:15:09.741755Z"
        },
        "id": "XtWTFfeUEDdn"
      },
      "outputs": [],
      "source": [
        "sns.histplot(x=WORDS_PER_TWEET, hue=TARGET, data=df_train, kde=True)\n",
        "plt.title('Tweet Length Analysis - Words')\n",
        "plt.legend([])\n",
        "plt.show()\n",
        "sns.histplot(x=CHAR_PER_TWEET, hue=TARGET, data=df_train, kde=True)\n",
        "plt.title('Tweet Analysis - Characters')\n",
        "plt.legend(labels=['Not about Real Disasters', 'About Real Disasters'], loc='best', bbox_to_anchor=(1.1, 0., 0.5, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNFw3R4lEDdn"
      },
      "source": [
        "# Tweet Text Analysis using WordCloud\n",
        "\n",
        "Word Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. Significant textual data points can be highlighted using a word cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:15:13.502234Z",
          "start_time": "2023-11-11T22:15:13.473899Z"
        },
        "id": "8DyzG4nkEDdn"
      },
      "outputs": [],
      "source": [
        "real_disaster_tweets = ' '. join(list(df_train[df_train[TARGET] == 1][TEXT]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:15:13.528428Z",
          "start_time": "2023-11-11T22:15:13.507802Z"
        },
        "id": "nTy7pNXNEDdq"
      },
      "outputs": [],
      "source": [
        "non_real_disaster_tweets = ' '. join(list(df_train[df_train[TARGET] == 0][TEXT]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:15:18.429964Z",
          "start_time": "2023-11-11T22:15:13.540986Z"
        },
        "id": "Bq0jX-x6EDdr"
      },
      "outputs": [],
      "source": [
        "wc = WordCloud(background_color=\"black\",\n",
        "               max_words=100,\n",
        "               width=1000,\n",
        "               height=600,\n",
        "               random_state=1).generate(real_disaster_tweets)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Wordcloud of Tweets about Real Disasters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:15:23.788701Z",
          "start_time": "2023-11-11T22:15:18.435985Z"
        },
        "id": "ro8-ku1jEDdr"
      },
      "outputs": [],
      "source": [
        "wc = WordCloud(background_color=\"black\",\n",
        "               max_words=100,\n",
        "               width=1000,\n",
        "               height=600,\n",
        "               font_step=1,\n",
        "               random_state=1).generate(non_real_disaster_tweets)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Wordcloud of Tweets NOT about Real Disasters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7M3ejdKEDds"
      },
      "source": [
        "Emojis are present in the text, as evident on the wordcloud. Therefore, they need to be either detected or removed. I will not be addressing emoji detection in this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JmRRFmnEDdt"
      },
      "source": [
        "# Location Analysis - SKIP\n",
        "\n",
        "Do tweets about real disasters have more standard locations which are detectable by NER?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:41.003699Z",
          "start_time": "2023-11-11T22:15:26.163825Z"
        },
        "id": "m_OERWqLEDdt"
      },
      "outputs": [],
      "source": [
        "def check_location(x):\n",
        "    ''' This method checks if the tweet location has any actual location\n",
        "    and saves them as as space-separated value if more than one.\n",
        "    If no location is found, then save blank'''\n",
        "    spacy_loc = nlp(x)\n",
        "    num_loc_in_tweet = len([ent.label_ for ent in spacy_loc.ents if ent.label_ == 'GPE'])\n",
        "    if num_loc_in_tweet:\n",
        "        locs_in_tweet = [ent.text for ent in spacy_loc.ents if ent.label_ == 'GPE']\n",
        "    else:\n",
        "        return [], 0\n",
        "    return locs_in_tweet, 1\n",
        "\n",
        "df_train[LOCATION].fillna('', inplace=True)\n",
        "df_train[LOCATIONS], df_train[IDENTIFIABLE_LOCATION] = zip(*df_train[LOCATION].apply(check_location))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:42.217797Z",
          "start_time": "2023-11-11T22:17:41.410604Z"
        },
        "id": "xRsiH1rWEDdt"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=TARGET, hue=IDENTIFIABLE_LOCATION, data=df_train, palette=['grey', 'red'])\n",
        "plt.legend(labels=['Location Unidentified', 'Location Identified'], loc='best', bbox_to_anchor=(1.1, 0., 0.5, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5jAXMwQEDdu"
      },
      "source": [
        "Tweets about real disasters have a slightly higher percentage of location identified by the named-entity identifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFQdH6_rEDdu"
      },
      "source": [
        "# Final Text Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:42.846749Z",
          "start_time": "2023-11-11T22:17:42.726317Z"
        },
        "id": "gJkuPL9aEDdu"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:42.996943Z",
          "start_time": "2023-11-11T22:17:42.864396Z"
        },
        "id": "WKnofXKwEDdu"
      },
      "outputs": [],
      "source": [
        "df_train[ALL_TEXT] = df_train[TEXT_TOKENIZED] + df_train[LOCATIONS]\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:43.027184Z",
          "start_time": "2023-11-11T22:17:43.008251Z"
        },
        "id": "tZAX64tbEDdv"
      },
      "outputs": [],
      "source": [
        "target = df_train[TARGET].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:43.088992Z",
          "start_time": "2023-11-11T22:17:43.041840Z"
        },
        "id": "gPwOv_-zEDdv"
      },
      "outputs": [],
      "source": [
        "df_train[ALL_TEXT_JOINED] = df_train[ALL_TEXT].apply(lambda x: \" \".join(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8FPWDh0EDdv"
      },
      "source": [
        "The training data is ready now. Next step, prepping the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rvYSAPyEDdv"
      },
      "source": [
        "# Building a Data Modeling Pipeline\n",
        "\n",
        "**Text Vectorizing** is a common method which converts a sequenceo of text to a sequence of numbers. The sequence of numbers could represent a token or a sentence, depending on your use-case.\n",
        "\n",
        "Count Vectors and Tf-idf are the most common methods of vectorizing texts.\n",
        "\n",
        "**Training a ML model** is the next step where the vectorized forms of texts are fed to a model, like logistic regression in this case. After model training, its performance is evaluated.\n",
        "\n",
        "**Classification Evaluation Metrics**\n",
        "\n",
        "1. Accuracy\n",
        "2. Precision\n",
        "3. Recall\n",
        "4. F1-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rUrEMzFEDdw"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:44.391836Z",
          "start_time": "2023-11-11T22:17:44.360989Z"
        },
        "id": "mdl59l9EEDdw"
      },
      "outputs": [],
      "source": [
        "def print_classification_metrics(y_train, train_pred, y_test, test_pred):\n",
        "    print('Training Accuracy: ', accuracy_score(y_train, train_pred))\n",
        "    print('Training f1-score: ', f1_score(y_train, train_pred))\n",
        "    print('Accuracy: ', accuracy_score(y_test, test_pred))\n",
        "    print('Precision: ', precision_score(y_test, test_pred))\n",
        "    print('Recall: ', recall_score(y_test, test_pred))\n",
        "    print('f1-score: ', f1_score(y_test, test_pred))\n",
        "\n",
        "def predict_challenge_test_data(model, test_data, filename):\n",
        "    submission_predictions = model.predict(test_data)\n",
        "    df_submission = pd.read_csv('data/sample_submission.csv')\n",
        "    df_submission[TARGET] = submission_predictions\n",
        "    df_submission.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUMkUvuUEDdw"
      },
      "source": [
        "## Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:49.901539Z",
          "start_time": "2023-11-11T22:17:44.760560Z"
        },
        "id": "Ye6xO8KzEDdx"
      },
      "outputs": [],
      "source": [
        "cols_to_train = [ALL_TEXT_JOINED]\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],\n",
        "                                                    df_train[TARGET].values,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "ct = ColumnTransformer([('count_vec',\n",
        "                         CountVectorizer(tokenizer=tt.tokenize,\n",
        "                                         ngram_range=(1, 2)),\n",
        "                                         ALL_TEXT_JOINED)],\n",
        "                       remainder='passthrough')\n",
        "\n",
        "ct.fit(X_train)\n",
        "X_train_sparse = ct.transform(X_train)\n",
        "X_test_sparse = ct.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:52.976155Z",
          "start_time": "2023-11-11T22:17:50.412647Z"
        },
        "id": "dCbtEi5yEDdx"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_sparse, y_train)\n",
        "test_prediction = log_reg.predict(X_test_sparse)\n",
        "training_prediction = log_reg.predict(X_train_sparse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:17:53.571146Z",
          "start_time": "2023-11-11T22:17:53.502775Z"
        },
        "id": "ynAX-3JtEDdy"
      },
      "outputs": [],
      "source": [
        "print_classification_metrics(y_train, training_prediction, y_test, test_prediction) # Replace this with scikitlearn impl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uEDiS7nEDdy"
      },
      "source": [
        "## TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:18:01.293400Z",
          "start_time": "2023-11-11T22:17:54.033376Z"
        },
        "id": "MFZJ4SRgEDdy"
      },
      "outputs": [],
      "source": [
        "cols_to_train = [ALL_TEXT_JOINED]\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],\n",
        "                                                    df_train[TARGET].values,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "ct = ColumnTransformer([('tfidf',\n",
        "                         TfidfVectorizer(tokenizer=tt.tokenize,\n",
        "                                         ngram_range=(1, 2),\n",
        "                                         smooth_idf=True), ALL_TEXT_JOINED)],\n",
        "                       remainder='passthrough')\n",
        "\n",
        "ct.fit(X_train)\n",
        "X_train_sparse = ct.transform(X_train)\n",
        "X_test_sparse = ct.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:18:03.361367Z",
          "start_time": "2023-11-11T22:18:01.992708Z"
        },
        "id": "65hO6zLyEDdz"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_sparse, y_train)\n",
        "test_prediction = log_reg.predict(X_test_sparse)\n",
        "training_prediction = log_reg.predict(X_train_sparse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:18:03.759401Z",
          "start_time": "2023-11-11T22:18:03.716237Z"
        },
        "id": "J6gHO6V0EDdz"
      },
      "outputs": [],
      "source": [
        "print_classification_metrics(y_train, training_prediction, y_test, test_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBcbBROMEDd0"
      },
      "source": [
        "## Experimenting with multiple classifiers - SKIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:18:04.041092Z",
          "start_time": "2023-11-11T22:18:04.022720Z"
        },
        "id": "8wxMNcm7EDd0"
      },
      "outputs": [],
      "source": [
        "def test_multiple_model(model_list):\n",
        "    for model in model_list:\n",
        "        cols_to_train = [ALL_TEXT_JOINED]\n",
        "\n",
        "        tt = TweetTokenizer()\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(df_train[cols_to_train],\n",
        "                                                            df_train[TARGET].values,\n",
        "                                                            test_size=0.2,\n",
        "                                                            random_state=42)\n",
        "\n",
        "        ct = ColumnTransformer([('tfidf',\n",
        "                                 TfidfVectorizer(tokenizer=tt.tokenize,\n",
        "                                                 ngram_range=(1, 2),\n",
        "                                                 smooth_idf=True), ALL_TEXT_JOINED)],\n",
        "                               remainder='passthrough')\n",
        "\n",
        "        ct.fit(X_train)\n",
        "        X_train_sparse = ct.transform(X_train)\n",
        "        X_test_sparse = ct.transform(X_test)\n",
        "        model.fit(X_train_sparse, y_train)\n",
        "        test_prediction = model.predict(X_test_sparse)\n",
        "        training_prediction = model.predict(X_train_sparse)\n",
        "        print(\"Model Name: \", type(model).__name__)\n",
        "        print_classification_metrics(y_train, training_prediction, y_test, test_prediction)\n",
        "        print(\"-------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:18:04.303073Z",
          "start_time": "2023-11-11T22:18:04.294362Z"
        },
        "id": "VKXnvOkAEDd1"
      },
      "outputs": [],
      "source": [
        "list_init_models = [LogisticRegression(),\n",
        "                    MultinomialNB(),\n",
        "                    DecisionTreeClassifier()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-11T22:18:32.274352Z",
          "start_time": "2023-11-11T22:18:04.539948Z"
        },
        "id": "qIgYw3AGEDd1"
      },
      "outputs": [],
      "source": [
        "test_multiple_model(list_init_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgUqf2wVEDd2"
      },
      "source": [
        "## Future Work\n",
        "\n",
        "1. Experiment with adding the features to the pipeline\n",
        "2. Experiment with emoji detection in the texts\n",
        "3. Tune the model for better performance\n",
        "4. Use word2vec on the texts and assess performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxaNxo7OEDd2"
      },
      "source": [
        "Thanks for visiting!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "353.299px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
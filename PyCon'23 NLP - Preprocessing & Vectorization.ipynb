{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f191906a",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6963e0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'after',\n",
       " 'as',\n",
       " 'best',\n",
       " 'big',\n",
       " 'biopic',\n",
       " 'everything',\n",
       " 'experience',\n",
       " 'format',\n",
       " 'gives',\n",
       " 'if',\n",
       " 'imax',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'made',\n",
       " 'must',\n",
       " 'of',\n",
       " 'only',\n",
       " 'oppenheimer',\n",
       " 'overall',\n",
       " 'perfect',\n",
       " 'possible',\n",
       " 'precise',\n",
       " 'reference',\n",
       " 'scientific',\n",
       " 'stadium',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'type',\n",
       " 'watch',\n",
       " 'with']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 100)\n",
    "\n",
    "example_texts = [\"It's actually the best scientific Biopic made after The theory of everything!!\",\n",
    "                \"Overall PERFECT , PRECISE WITH B/W REFERENCE, OPPENHEIMER IS A MUST WATCH'ONLY IN IMAX FORMAT IF POSSIBLE AS IT GIVES A BIG STADIUM TYPE EXPERIENCE.\"]\n",
    "vectorizer.fit(example_texts)\n",
    "\n",
    "# Top-10 words\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e4682c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually</th>\n",
       "      <th>after</th>\n",
       "      <th>as</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>biopic</th>\n",
       "      <th>everything</th>\n",
       "      <th>experience</th>\n",
       "      <th>format</th>\n",
       "      <th>gives</th>\n",
       "      <th>...</th>\n",
       "      <th>possible</th>\n",
       "      <th>precise</th>\n",
       "      <th>reference</th>\n",
       "      <th>scientific</th>\n",
       "      <th>stadium</th>\n",
       "      <th>the</th>\n",
       "      <th>theory</th>\n",
       "      <th>type</th>\n",
       "      <th>watch</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   actually  after  as  best  big  biopic  everything  experience  format  \\\n",
       "0         1      1   0     1    0       1           1           0       0   \n",
       "1         0      0   1     0    1       0           0           1       1   \n",
       "\n",
       "   gives  ...  possible  precise  reference  scientific  stadium  the  theory  \\\n",
       "0      0  ...         0        0          0           1        0    2       1   \n",
       "1      1  ...         1        1          1           0        1    0       0   \n",
       "\n",
       "   type  watch  with  \n",
       "0     0      0     0  \n",
       "1     1      1     1  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_example_texts = vectorizer.transform(example_texts)\n",
    "\n",
    "data = pd.DataFrame(vectorized_example_texts.toarray())\n",
    "data.columns = vectorizer.get_feature_names()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea16b0f",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency – Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dfea06e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['actually', 'after', 'as', 'best', 'big', 'biopic', 'everything',\n",
       "       'experience', 'format', 'gives', 'if', 'imax', 'in', 'is', 'it',\n",
       "       'made', 'must', 'of', 'only', 'oppenheimer', 'overall', 'perfect',\n",
       "       'possible', 'precise', 'reference', 'scientific', 'stadium', 'the',\n",
       "       'theory', 'type', 'watch', 'with'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Fit TF-IDF on train texts\n",
    "vectorizer = TfidfVectorizer(max_features = 200, norm = None) # take top 200 words\n",
    "vectorizer.fit(example_texts)\n",
    "\n",
    "# Top-10 words\n",
    "vectorizer.get_feature_names_out()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e4cb9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actually</th>\n",
       "      <th>after</th>\n",
       "      <th>as</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>biopic</th>\n",
       "      <th>everything</th>\n",
       "      <th>experience</th>\n",
       "      <th>format</th>\n",
       "      <th>gives</th>\n",
       "      <th>...</th>\n",
       "      <th>possible</th>\n",
       "      <th>precise</th>\n",
       "      <th>reference</th>\n",
       "      <th>scientific</th>\n",
       "      <th>stadium</th>\n",
       "      <th>the</th>\n",
       "      <th>theory</th>\n",
       "      <th>type</th>\n",
       "      <th>watch</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.81093</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>...</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   actually     after        as      best       big    biopic  everything  \\\n",
       "0  1.405465  1.405465  0.000000  1.405465  0.000000  1.405465    1.405465   \n",
       "1  0.000000  0.000000  1.405465  0.000000  1.405465  0.000000    0.000000   \n",
       "\n",
       "   experience    format     gives  ...  possible   precise  reference  \\\n",
       "0    0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.000000   \n",
       "1    1.405465  1.405465  1.405465  ...  1.405465  1.405465   1.405465   \n",
       "\n",
       "   scientific   stadium      the    theory      type     watch      with  \n",
       "0    1.405465  0.000000  2.81093  1.405465  0.000000  0.000000  0.000000  \n",
       "1    0.000000  1.405465  0.00000  0.000000  1.405465  1.405465  1.405465  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_example_texts_tf_idf = vectorizer.transform(example_texts)\n",
    "\n",
    "data = pd.DataFrame(vectorized_example_texts_tf_idf.toarray())\n",
    "data.columns = vectorizer.get_feature_names_out()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11346881",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33cba3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # library for working with strings\n",
    "import nltk   # Natural Language Toolkit\n",
    "import numpy as np # Library for numerical, matrix computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3eb19524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lakhty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the list of stopwords for English\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# example of stop words\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a1d58a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punctuation\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "392e970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize the WordPunctTokenizer, with which we will then split the text into words.\n",
    "word_tokenizer = nltk.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "31d4e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [str(x) for x in np.arange(1900, 2022)]\n",
    "def preprocess_data(data):\n",
    "    texts = []\n",
    "    targets = []\n",
    "    \n",
    "    # iterate over reviews\n",
    "    for item in data:\n",
    "               \n",
    "        text_lower = item.lower() # lowercase texts\n",
    "        tokens     = word_tokenizer.tokenize(text_lower) #split text into words\n",
    "        \n",
    "        # remove punctuation and stop words\n",
    "        tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words and word not in dates)]\n",
    "        texts.append(tokens) # add into the list\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "db62064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = preprocess_data(example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38285255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['actually', 'best', 'scientific', 'biopic', 'made', 'theory', 'everything', '!!']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens: \", texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b53395ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's actually the best scientific Biopic made after The theory of everything\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation removal - regex\n",
    "import re\n",
    "\n",
    "clean_text = re.sub(r'!+', '', example_texts[0])\n",
    "clean_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "692d31b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word '!' appears 2 times in the text.\n"
     ]
    }
   ],
   "source": [
    "# example of feature for the sentiment classification task\n",
    "def count_word_occurrences(text, word):\n",
    "    word_count = text.count(word)\n",
    "    return word_count\n",
    "\n",
    "word = \"!\"\n",
    "\n",
    "result = count_word_occurrences(example_texts[0], word)\n",
    "print(f\"The word '{word}' appears {result} times in the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5fc19b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "767a28a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: It's, After: it\n",
      "Before: actually, After: actual\n",
      "Before: the, After: the\n",
      "Before: best, After: best\n",
      "Before: scientific, After: scientif\n",
      "Before: Biopic, After: biopic\n",
      "Before: made, After: made\n",
      "Before: after, After: after\n",
      "Before: The, After: the\n",
      "Before: theory, After: theori\n",
      "Before: of, After: of\n",
      "Before: everything, After: everyth\n"
     ]
    }
   ],
   "source": [
    "# examples of stemming using NLTK\n",
    "for word in clean_text.split():\n",
    "    word_stem = stemmer.stem(word)\n",
    "    print(\"Before: %s, After: %s\" % (word, word_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ea5829c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: It's.\t\t\tAfter: It's\n",
      "Before: actually.\t\t\tAfter: actually\n",
      "Before: the.\t\t\tAfter: the\n",
      "Before: best.\t\t\tAfter: best\n",
      "Before: scientific.\t\t\tAfter: scientific\n",
      "Before: Biopic.\t\t\tAfter: Biopic\n",
      "Before: made.\t\t\tAfter: made\n",
      "Before: after.\t\t\tAfter: after\n",
      "Before: The.\t\t\tAfter: The\n",
      "Before: theory.\t\t\tAfter: theory\n",
      "Before: of.\t\t\tAfter: of\n",
      "Before: everything.\t\t\tAfter: everything\n"
     ]
    }
   ],
   "source": [
    "# examples of lemmatization using NLTK\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in clean_text.split():\n",
    "    word_norm = lemmatizer.lemmatize(word)\n",
    "    print(\"Before: %s.\\t\\t\\tAfter: %s\" % (word, word_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e0c9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/lakhty/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27325a2c",
   "metadata": {},
   "source": [
    "### Last but Not Least ... spaCy 🤗 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a81c78a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: It \n",
      "POS: PRON\n",
      "Lemma: it\n",
      "\n",
      "Entity: 's \n",
      "POS: AUX\n",
      "Lemma: be\n",
      "\n",
      "Entity: actually \n",
      "POS: ADV\n",
      "Lemma: actually\n",
      "\n",
      "Entity: the \n",
      "POS: DET\n",
      "Lemma: the\n",
      "\n",
      "Entity: best \n",
      "POS: ADV\n",
      "Lemma: well\n",
      "\n",
      "Entity: scientific \n",
      "POS: ADJ\n",
      "Lemma: scientific\n",
      "\n",
      "Entity: Biopic \n",
      "POS: PROPN\n",
      "Lemma: Biopic\n",
      "\n",
      "Entity: made \n",
      "POS: VERB\n",
      "Lemma: make\n",
      "\n",
      "Entity: after \n",
      "POS: ADP\n",
      "Lemma: after\n",
      "\n",
      "Entity: The \n",
      "POS: DET\n",
      "Lemma: the\n",
      "\n",
      "Entity: theory \n",
      "POS: NOUN\n",
      "Lemma: theory\n",
      "\n",
      "Entity: of \n",
      "POS: ADP\n",
      "Lemma: of\n",
      "\n",
      "Entity: everything \n",
      "POS: PRON\n",
      "Lemma: everything\n",
      "\n",
      "Entity: ! \n",
      "POS: PUNCT\n",
      "Lemma: !\n",
      "\n",
      "Entity: ! \n",
      "POS: PUNCT\n",
      "Lemma: !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(example_texts[0])\n",
    "entities = [(ent.text, ent.pos_, ent.lemma_) for ent in doc]\n",
    "\n",
    "for entity, pos, lemma in entities:\n",
    "    print(f\"Entity: {entity} \\nPOS: {pos}\\nLemma: {lemma}\")\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "793fee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4aba6",
   "metadata": {},
   "source": [
    "### ✨ Bonus materials ✨\n",
    "\n",
    "**Useful NLP sources**:\n",
    "\n",
    "- spaCy documentation: https://spacy.io/\n",
    "- excellent article on Medium about NLP: https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e\n",
    "- NLP overview: https://tfduque.medium.com/dissecting-natural-language-processing-layer-by-layer-an-introductory-overview-d11cfff4f329\n",
    "\n",
    "**Advanced**:\n",
    "- NLP Newsletter by Sebastian Ruder: https://www.ruder.io/nlp-news/\n",
    "- The Batch: https://read.deeplearning.ai/the-batch/\n",
    "- MLOps: https://mlops.substack.com/\n",
    "- Data Is Plural: https://www.data-is-plural.com/\n",
    "- Reddit: https://www.reddit.com/r/MachineLearning/ https://www.reddit.com/r/LanguageTechnology/\n",
    "- NLP Newsletter by Elvis https://substack.com/@elvissaravia - https://dair.ai/newsletter/\n",
    "- Import AI https://jack-clark.net/\n",
    "- The Gradient https://thegradient.pub/\n",
    "- The Medical Futurist https://medicalfuturist.com/\n",
    "\n",
    "\n",
    "**Twitter sources**:\n",
    "\n",
    "@omarsar0\n",
    "@svpino\n",
    "@davisblalock\n",
    "@mervenoyann\n",
    "@jeremyphoward\n",
    "@paperswithcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8276a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
